# Correlation analysis among variables

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(results = 'asis')
```

```{r path}
path <- "/Users/austin.argentieri/OneDrive - Nexus365/DPhil Research/UKB/Analysis projects/ACM EWAS 2021"

# load analysis datasets
load(paste0(path, "/datasets/ACM_final_analysis_datasets_feb_16_2022.RData"))
```

```{r hetcor}
# load final datasets

library(polycor)
library(parallel)
library(Hmisc)

# set cores for parallel computation
cores <- detectCores(logical = FALSE) - 1

### merge male/female datasets -------------------------------------------------------------------

# recode names of female variables to make match with men
for (k in seq_along(fmult_disc)) {
    fmult_disc[[k]]$insulin <- fmult_disc[[k]]$insulin_v2
    fmult_disc[[k]]$blood_pressure_meds <- fmult_disc[[k]]$blood_pressure_meds_v2
    fmult_disc[[k]]$cholesterol_meds <- fmult_disc[[k]]$cholesterol_meds_v2
}

# get vector of common columns (vars) between men and women
common_cols <- intersect(colnames(mult_disc[[1]]), colnames(fmult_disc[[1]]))

# function to subset datasets to common columns
common_col_subset <- function(x) {
    x <- x[common_cols]
    return(x)
}

# run in datasets
mult_disc_common <- lapply(mult_disc, common_col_subset)
fmult_disc_common <- lapply(fmult_disc, common_col_subset)

# rbind male and female discovery datasets
discovery_combined <- as.list(seq_along(mult_disc))
for (k in seq_along(discovery_combined)) {
    discovery_combined[[k]] <- rbind(mult_disc_common[[k]],
                                     fmult_disc_common[[k]])
}

### Setting exposures -------------------------------------------------------------------

# create vector of exposure names
exposures <- as.vector(colnames(discovery_combined[[1]]))

# make list of variables in dataset to exclude from analysis as exposures (including covariates in model)
exclude <- c(
    Cs(
        eid, # participant ID
        hazard, # outcome
        ACM_survival_time, # outcome
        birth_year, # used for creation of birth cohort strata
        recruitment_date, # recruitment metric only

        alcohol_status, # only used to create NA for alcohol_freq
        hand_grip_strength_right, # old var before normalization
        hand_grip_strength_left, # old var before normalization
        FEV1_best, # old var before normalization
        FVC_best, # old var before normalization
        sleep_hours, # old var before normalization
        education_college, # redundant to covariate
        education_A_levels, # redundant to covariate          
        education_O_levels, # redundant to covariate
        education_CSEs, # redundant to covariate             
        education_NVQ, # redundant to covariate
        education_other_professional, # redundant to covariate
        education_none, # redundant to covariate               

        cochlear_implant, # remove after crosstab QC
        portable_gas_heat, # remove after crosstab QC
        solid_fuel_heat, # remove after crosstab QC
        open_fire_no_central_heat, # remove after crosstab QC
        hshld_grandparent, # remove after crosstab QC
        hshld_other_related, # remove after crosstab QC
        student, # remove after crosstab QC
        trauma_vision_loss, # remove after crosstab QC
        pregnant, # remove after crosstab QC
        other_diagnosis, # not interpretable
        other_eye_problems, # not interpretable
        other_meds, # not interpretable
        other_group_activity, # not interpretable
        other_serious_eye_condition, # not interpretable
        
        tobacco_exposure_outside, # nested
        tobacco_exposure_home, # nested
        hshld_smokers, # nested
        heel_sound_speed_right, # nested
        employment_distance, # nested
        employment_years, # nested
        employment_hours, # nested
        employment_travel_freq, # nested
        car_commute, # nested
        walk_commute, # nested
        public_transport_commute, # nested
        cycle_commute, # nested
        employment_standing, # nested
        employment_heavy_manual, # nested
        shift_work, # nested
        night_shift_work, # nested
        noisy_workplace # nested
    )
)

# remove variables I don't want to test in EWAS as exposures
exposures <- exposures[exposures %nin% exclude]

# add in categories to sort
exposures <- as.data.frame(exposures) # create df from exposure list
summary <- merge(exposures,
                 categories,
                 by.x = "exposures",
                 by.y = "X...Variable",
                 all = F) # merge with categories
summary <- summary[order(summary$Category), ] # order exposures by category
exposures <- as.vector(summary$exposures) # make final vector of exposures ordered by category
exposures <- c(exposures, "ACM_event_indicator")

### Correlation -------------------------------------------------------------------

# set indicator as factor
make_factor <- function(x) {
    x$ACM_event_indicator <- factor(x$ACM_event_indicator, 
                                    ordered = FALSE)
    return(x)
}

discovery_combined <- lapply(discovery_combined, make_factor)

# subset imputed dfs to EWAS exposures
corr_dfs <- lapply(discovery_combined, function(x) x <- x[, exposures])

# function for hetcor
run_hetcor <- function(x) {
  hetcor(x, std.err = FALSE, use = "pairwise.complete.obs")
}

# run hetcor on each imputed dataset
corr <- mclapply(corr_dfs,
                 run_hetcor,
                 mc.cores = cores)

# save
save(corr, file = "/path/to/file/hetcor_output_feb_22_2022_all_sexes.RData")
```

``` {r corr_pool, eval=FALSE}

## pool correlation coefficients across multiple imputed datasets

# load hetcor results
load(paste0(path, "/results/correlation/hetcor_output_feb_22_2022_all_sexes.RData"))

library(DescTools)

# set global options
options(scipen = 999)

# extract matrices
corr <- lapply(corr, function(x) x[["correlations"]])

### transform coefficients using fisher's z scale transformation

# run Fisher transformation
corr_zs <- lapply(corr, FisherZ)

# replace inf with 100 (will re-transform back to 1)
corr_zs[[1]][is.infinite(corr_zs[[1]])] <- 100
corr_zs[[2]][is.infinite(corr_zs[[2]])] <- 100
corr_zs[[3]][is.infinite(corr_zs[[3]])] <- 100
corr_zs[[4]][is.infinite(corr_zs[[4]])] <- 100
corr_zs[[5]][is.infinite(corr_zs[[5]])] <- 100

# pool coefficients (which is just the average)
tmp <- Reduce('+', corr_zs)
corr_zs_pool <- tmp / length(corr_zs)

# transform coefficients back to r scale
corr_pool <- FisherZInv(corr_zs_pool)

### calculate p-value for all coefficients using z transformed coefficients and standard error. 
# See here for code suggestion: 
# https://stats.stackexchange.com/questions/61026/can-p-values-for-pearsons-correlation-test-be-computed-just-from-correlation-co

zse <- 1 / sqrt(nrow(discovery_combined[[1]]) - 3) # standard error using discovery sample size
p.val_corr <- apply(corr_zs_pool, # z-transformed correlation coefficients
                    c(1,2), # apply below formula to cells in all rows and columns
                    function(x) min(pnorm(x, sd = zse),
                                    pnorm(x, lower.tail = F, sd = zse)) * 2) 

# save
save(corr_pool, 
     p.val_corr, 
     file = paste0(path, "/results/correlation/pooled_correlation_feb_22_2022_all_sexes.RData"))
```

```{r cluster_number}

## testing number of clusters

# load XWAS results
load(paste0(path,"/results/full cohort/ACM_XWAS_results_feb_22_2022_all_sexes.RData"))

# load pooled correlation results
load(paste0(path, "/results/correlation/pooled_correlation_feb_22_2022_all_sexes.RData"))

library(Hmisc)
library(factoextra)

# get vector of variables significant in XWAS + covariates
exposures <- XWAS_total[which(XWAS_total$FDR.rep < 0.05), ]
exposures <- unique(exposures$Variable)
exposures <- c(
  exposures,
  Cs(
    education_years,
    hshld_income,
    ethnicity,
    recruitment_centre
  )
)

# subset correlation matrix to variables significant in XWAS
corr_pool <- as.data.frame(corr_pool)

corr_pool_sig <- corr_pool[ ,colnames(corr_pool) %in% exposures]
corr_pool_sig <- corr_pool_sig[rownames(corr_pool_sig) %in% exposures, ]

corr_pool_sig <- as.matrix(corr_pool_sig)
corr_pool_sig <- round(corr_pool_sig, digits = 2)

## Determining optimanl number of clusters by testing AIC of differnt K-means clustering runs
## Taken from: http://sherrytowers.com/2013/10/24/k-means-clustering/ 

# run kmeans clustering from 1 - 100 clusters
kmax = 100 # the maximum number of clusters we will examine; you can change this
totwss = rep(0, kmax) # will be filled with total sum of within group sum squares
kmfit = list() # create and empty list

set.seed(3456)
for (i in 1:kmax){
kclus = kmeans(corr_pool_sig,
               centers = i,
               iter.max = 20)
totwss[i] = kclus$tot.withinss
kmfit[[i]] = kclus
}

# function to calculate AIC
kmeansAIC = function(fit){

m = ncol(fit$centers)
n = length(fit$cluster)
k = nrow(fit$centers)
D = fit$tot.withinss
return(D + 2*m*k)

}

# run AIC
aic = sapply(kmfit, kmeansAIC)

# calculate optimal # of clusters and add to plot
v = -diff(aic)
nv = length(v)
fom = v[1:(nv-1)]/v[2:nv]
nclus = which.max(fom) + 1 # optimal number of clusters
cat("The apparent number of clusters is: ", nclus, "\n")

# plot and save
setwd(paste0(path, "/output/correlation"))
pdf(paste0(paste0("AIC_exposome_", nclus), "_clusters_feb_22_2022_all_sexes.pdf"), 
    height = 7, 
    width = 15) 
# initialize plot (must run all code from here to end of chunk at same time)
plot(
    seq(1, kmax),
    aic,
    xlab = "Number of clusters k",
    ylab = "AIC of each cluster model",
    main = "Optimal Number of Clusters in Men - Exposome Variables",
    font.main = 1,
    pch = 20,
    cex = 2
) 
# axis scale and intervals
axis(side = 1, at = seq(0, 100, by = 2))
# vertical line for number of clusters
abline(v = nclus, col = "black", lwd = 1, lty = 2)
# extra plot info
points(nclus,
       aic[nclus],
       col = 2,
       pch = 20,
       cex = 2)
dev.off()

## visualize optimal number of clusters using elbow method (sum of WSS)
## More tips here: https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/

elbow <- fviz_nbclust(corr_pool_sig, 
                      hcut, 
                      k.max = 100,
                      method = "wss") +
    geom_vline(xintercept = nclus, linetype = 2, color = "black") +
    geom_vline(xintercept = 36, linetype = 2, color = "red") +
    labs(subtitle = "Elbow method")

elbow +
    scale_x_discrete(breaks = seq(0,150,5))

setwd(paste0(path, "/output/correlation"))
pdf(paste0(paste0("elbow_exposome_", nclus), "_clusters_feb_22_2022_all_sexes.pdf"), 
    height = 7, 
    width = 15) 
elbow +
    scale_x_discrete(breaks = seq(0,100,2)) +
    scale_y_continuous(breaks = seq(0,750,50)) +
    labs(y = "Total within-cluster sum of squares (WSS)") +
    theme(axis.title = element_text(size = 12)) +
    ggtitle("Optimal Number of Clusters - Exposome Variables")
dev.off()
```

```{r circular_dendro}

## create circular dendrogram (not shown in manuscript)
library(randomcoloR)
library(dendextend)
library(dplyr)

# set number of clusters
# nclus <- nclus
nclus <- 38

# generate color for each category of variable
# try different random seeds until you like the results

set.seed(1234)
pal4 <- c(distinctColorPalette(k = nclus, altCol = FALSE, runTsne = FALSE))

### Hierarchical clustering
set.seed(6789)
hr <- dist(corr_pool_sig, method = "euclidean")
fit <- hclust(hr, method = "complete")
dend <- as.dendrogram(fit)

### create and save circular dendrogram
setwd(paste0(path, "/output/correlation"))
png(paste0(paste0("circular_dendogram_exposome_", nclus), "_clusters_feb_22_2022_all_sexes.png"), 
    width = 15,
    height = 15,
    units = "in", 
    res = 1200) 
par(oma = c(10, 3, 10, 3)) # all sides have 3 lines of space
par(mar = rep(1,4), xpd = NA)
# par(mar=c(5,5,5,5)+.1)
dend %>% 
  set("labels_cex", 1) %>% 
  # set("branches_col", value = c(3,4), k = 2) %>% 
  set("branches_k_color", value = pal4, k = nclus) %>%
  set("branches_lwd", 3) %>% 
  circlize_dendrogram(dend_track_height = .8,
                      labels_track_height = .1)

dev.off()

```

```{r save_clusters}

## save csv file of variables in all clusters

# get df of clusters
groups <- cutree(fit, k = nclus)

# convert to df
mydata <- data.frame(groups)
  
# make column with variable names
mydata$Variable <- rownames(mydata)

# set column names
colnames(mydata) <- c("Cluster", "Variable")

# create list to store pool results
prefix <- "cluster"
suffix <- seq(1,nclus)
clusters <- as.list(paste0(prefix, suffix))

# loop to create list of each cluster
for(j in seq_along(clusters)){
 clusters[[j]] <-  mydata[mydata$Cluster == j,]
}

# remove cluster number
clusters <- lapply(clusters, function(x) x <- subset(x, select = c("Variable")))

# allow to be df with different length (need it double)
clusters <- sapply(clusters, "length<-", max(lengths(clusters)))
clusters <- sapply(clusters, "length<-", max(lengths(clusters)))
clusters <- as.data.frame(clusters)

prefix <- "Cluster"
suffix <- seq(1,nclus)
cluster_names <- paste(prefix, suffix, sep = " ")

colnames(clusters) <- c(cluster_names)

write.csv(clusters, 
          file = paste0(path, paste0(paste0("/output/correlation/hierarchical_exposome_", nclus), "_clusters_feb_22_2022_all_sexes.csv")))

```

```{r save_html_correlation_matrix_all_vars}

## save HTML table with correlation between all variables

# rename
corr <- corr_pool
corr <- as.matrix(corr)
corr <- round(corr, digits = 2)

#prepare to drop duplicates and correlations of 1     
corr[lower.tri(corr,diag = TRUE)] <- NA 
#turn into a 3-column table
corr <- as.data.frame(as.table(corr))
#remove the NA values from above 
corr <- na.omit(corr) 

# rename
p_sig <- p.val_corr

# Keep only upper triangle (bottom triangle are all duplicates)
p_sig[lower.tri(p_sig, diag = TRUE)] <- NA 
#turn into a 3-column table
p_sig <- as.data.frame(as.table(p_sig))
#remove the NA values from above 
p_sig <- na.omit(p_sig) 

corr <- cbind(corr, p_sig)
corr <- corr[, -(4:5)]

#sort by highest correlation
corr <- corr[order(-abs(corr$Freq)),] 

colnames(corr) <- c("Variable 1", "Variable 2", "Coefficient", "p-value")

library(DT)
plot1 <- datatable(corr,rownames = FALSE, fillContainer = T,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:2)),
                         scrollY = '600px',
                         scrollx = '100%',
                         pageLength = 25),
          caption = "Baseline Variable Correlation - UK Biobank") %>%
    formatRound(columns=c("p-value"), digits = 5)

# save
saveWidget(plot1, 
           file = paste0(path, "/output/correlation/html/correlation_table_all_vars_feb_22_2022_all_sexes.html"))

```

```{r save_html_correlation_matrix_exposome}

## save HTML table with correlation between just exposome variables

# get vector of exposome variables
full_exposures <- unique(XWAS_total$Variable)
full_exposures <- c(
  full_exposures,
  Cs(
    education_years,
    hshld_income,
    ethnicity,
    recruitment_centre
  )
)

# subset correlation matrix to exposome variables
corr_pool <- as.data.frame(corr_pool)
corr_pool_exp <- corr_pool[ ,colnames(corr_pool) %in% full_exposures]
corr_pool_exp <- corr_pool_exp[rownames(corr_pool_exp) %in% full_exposures, ]

corr_pool_exp <- as.matrix(corr_pool_exp)
corr_pool_exp <- round(corr_pool_exp, digits = 2)

# rename
corr <- corr_pool_exp

#prepare to drop duplicates and correlations of 1     
corr[lower.tri(corr,diag = TRUE)] <- NA 
#turn into a 3-column table
corr <- as.data.frame(as.table(corr))
#remove the NA values from above 
corr <- na.omit(corr) 

# rename
p_sig <- p.val_corr[rownames(corr_pool_exp), colnames(corr_pool_exp)]

# Keep only upper triangle (bottom triangle are all duplicates)
p_sig[lower.tri(p_sig, diag = TRUE)] <- NA 
#turn into a 3-column table
p_sig <- as.data.frame(as.table(p_sig))
#remove the NA values from above 
p_sig <- na.omit(p_sig) 

corr <- cbind(corr, p_sig)
corr <- corr[, -(4:5)]

#sort by highest correlation
corr <- corr[order(-abs(corr$Freq)),] 

colnames(corr) <- c("Variable 1", "Variable 2", "Coefficient", "p-value")

library(DT)
plot1 <- datatable(corr,rownames = FALSE, fillContainer = T,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:2)),
                         scrollY = '600px',
                         scrollx = '100%',
                         pageLength = 25),
          caption = "Exposome Variable Correlation - UK Biobank") %>%
    formatRound(columns=c("p-value"), digits = 5)

# save
saveWidget(plot1, 
           file = paste0(path, "/output/correlation/html/correlation_table_exposome_feb_22_2022_all_sexes.html"))
```

```{r sig_correlations_count}

## count number of significant correlations by coefficient and p-value

## Transform correlation and p-value matrices

p_sig <- p.val_corr[rownames(corr_pool_sig),colnames(corr_pool_sig)]

# Keep only upper triangle (bottom triangle are all duplicates)
p_sig[lower.tri(p_sig, diag = TRUE)] <- NA
#turn into a 3-column table
p_sig <- as.data.frame(as.table(p_sig))
#remove the NA values from above 
p_sig <- na.omit(p_sig) 

corr <- corr_pool_sig
corr <- as.matrix(corr)

# Keep only upper triangle (bottom triangle are all duplicates)
corr[lower.tri(corr, diag = TRUE)] <- NA 
#turn into a 3-column table
corr <- as.data.frame(as.table(corr))
#remove the NA values from above 
corr <- na.omit(corr) 

bonferroni <- 0.05/(nrow(corr))

## get number of correlations by p-value significance

#select significant values  
p_sig.05 <- subset(p_sig, Freq < 0.05) 
p_sig.01 <- subset(p_sig, Freq < 0.01) 
p_sig.0001 <- subset(p_sig, Freq < 0.001) 
p_sig.bon <- subset(p_sig, Freq < bonferroni)

# calculate percent significantly correlated
percent_05 <- nrow(p_sig.05)/nrow(p_sig)
percent_01 <- nrow(p_sig.01)/nrow(p_sig)
percent_0001 <- nrow(p_sig.0001)/nrow(p_sig)
percent_bon <- nrow(p_sig.bon)/nrow(p_sig)

## get mean and median correlation coefficients by p-value significance

# calculate means 
corr_05 <- mean(abs(corr[rownames(p_sig.05),]$Freq))
corr_01 <- mean(abs(corr[rownames(p_sig.01),]$Freq))
corr_0001 <- mean(abs(corr[rownames(p_sig.0001),]$Freq))
corr_bon <- mean(abs(corr[rownames(p_sig.bon),]$Freq))

# calculate medians
med_05 <- median(abs(corr[rownames(p_sig.05),]$Freq))
med_01 <- median(abs(corr[rownames(p_sig.01),]$Freq))
med_0001 <- median(abs(corr[rownames(p_sig.0001),]$Freq))
med_bon <- median(abs(corr[rownames(p_sig.bon),]$Freq))

percent <- c(percent_05,percent_01,percent_0001,percent_bon)
cors <- c(corr_05,corr_01,corr_0001,corr_bon)
meds <- c(med_05,med_01,med_0001,med_bon)

# count correlations above 0.3
length <- ncol(corr_pool_sig)
pairs <- length * length
counts <- lapply(corr_pool_sig, function(x) abs(x) > 0.3)
sigs <- length(counts[counts == TRUE])
sig_pairs <- sigs/pairs

# count correlations above 0.5
counts_05 <- lapply(corr_pool_sig, function(x) abs(x) > 0.5)
sigs_05 <- length(counts_05[counts_05 == TRUE])
sig_pairs_05 <- sigs_05/pairs

# create table
table <- data.frame(matrix(NA, nrow = 6, ncol = 3))
table[1:4, 1] <- cors
table[1:4, 2] <- meds
table[1:4, 3] <- percent*100
table[5, 3] <- sig_pairs*100
table[6, 3] <- sig_pairs_05*100
# round to 2 decimals and force to keep 2 decimals (e.g., 0.80 and not 0.8)
table <- format(round(table, digits = 2), nsmall = 2) 

# set label names                   
rownames(table) <- c("p < 0.05", "p < 0.01", "p < 0.0001", "Bonferroni", "Coefficient above 0.30", "Coefficient above 0.50")

# set column names
colnames(table) <- c("Mean coefficient (abs)", "Median coefficient (abs)", "Percent of correlations")

# format percentage column
table$"Percent of correlations" <- paste(table$"Percent of correlations"," %")

# save
write.csv(table, 
          file = paste0(path, "/output/correlation/exposome_correlation_significance_all_sexes.csv"))

library(DT)
datatable(table,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:3))))

```

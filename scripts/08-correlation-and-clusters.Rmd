
### Correlation and clustering analysis

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(results = 'asis')
```

```{r path}
path <- "/path/to/file"

# load analysis datasets
load(paste0(path, "/datasets/ACM_final_analysis_datasets_aug_20_2022_all_data.RData"))

```

## Run correlation

```{r hetcor}

# load variable categories
categories <- read.csv("/path/to/file/Argentieri_UKB_dataset_data_dictionary.csv")

library(polycor)
library(parallel)
library(Hmisc)

# set cores for parallel computation
cores <- detectCores(logical = FALSE) - 1

### Setting exposures -------------------------------------------------------------------

# create vector of exposure names
exposures <- as.vector(colnames(all_data[[1]]))

# make list of variables in dataset to exclude from analysis as exposures (including covariates in model)
exclude <- c(
    Cs(
        eid, # participant ID
        hazard, # outcome
        ACM_survival_time, # outcome
        birth_year, # used for creation of birth cohort strata
        recruitment_date, # recruitment metric only

        alcohol_status, # only used to create NA for alcohol_freq
        hand_grip_strength_right, # old var before normalization
        hand_grip_strength_left, # old var before normalization
        FEV1_best, # old var before normalization
        FVC_best, # old var before normalization
        sleep_hours, # old var before normalization
        education_college, # redundant to covariate
        education_A_levels, # redundant to covariate
        education_O_levels, # redundant to covariate
        education_CSEs, # redundant to covariate
        education_NVQ, # redundant to covariate
        education_other_professional, # redundant to covariate
        education_none, # redundant to covariate

        cochlear_implant, # remove after crosstab QC
        portable_gas_heat, # remove after crosstab QC
        solid_fuel_heat, # remove after crosstab QC
        open_fire_no_central_heat, # remove after crosstab QC
        hshld_grandparent, # remove after crosstab QC
        hshld_other_related, # remove after crosstab QC
        student, # remove after crosstab QC
        trauma_vision_loss, # remove after crosstab QC
        pregnant, # remove after crosstab QC
        other_diagnosis, # not interpretable
        other_eye_problems, # not interpretable
        other_meds, # not interpretable
        other_group_activity, # not interpretable
        other_serious_eye_condition, # not interpretable

        tobacco_exposure_outside, # nested
        tobacco_exposure_home, # nested
        hshld_smokers, # nested
        heel_sound_speed_right, # nested
        employment_distance, # nested
        employment_years, # nested
        employment_hours, # nested
        employment_travel_freq, # nested
        car_commute, # nested
        walk_commute, # nested
        public_transport_commute, # nested
        cycle_commute, # nested
        employment_standing, # nested
        employment_heavy_manual, # nested
        shift_work, # nested
        night_shift_work, # nested
        noisy_workplace # nested
    )
)

# remove variables I don't want to test
exposures <- exposures[exposures %nin% exclude]

# add in categories to sort
exposures <- as.data.frame(exposures) # create df from exposure list
summary <- merge(exposures,
                 categories,
                 by.x = "exposures",
                 by.y = names(categories)[1],
                 all = FALSE)

# remove sex-specific variables
sex_vars <- which(summary$Category %in% c("Female reproductive factors", "Male reproductive factors"))
summary <- summary[-sex_vars, ]

# order exposures by category
summary <- summary[order(summary$Category), ] 

# make final vector of exposures ordered by category
exposures <- as.vector(summary$exposures) 
exposures <- c(exposures, "ACM_event_indicator")

### Correlation -------------------------------------------------------------------

# set mortality indicator as factor
make_factor <- function(x) {
    x$ACM_event_indicator <- factor(x$ACM_event_indicator, 
                                    ordered = FALSE)
    return(x)
}

all_data <- lapply(all_data, make_factor)

# subset imputed dfs to EWAS exposures
corr_dfs_all_data <- lapply(all_data, function(x) x <- x[, exposures])

# function for hetcor
run_hetcor <- function(x) {
  hetcor(x, std.err = FALSE, use = "pairwise.complete.obs")
}

# run hetcor on each imputed dataset
corr_all_data <- mclapply(corr_dfs_all_data,
                          run_hetcor,
                          mc.cores = cores)

# save
save(corr_all_data, file = "/path/to/file/hetcor_output_aug_10_2022_all_data.RData")

```

## Pool correllation across imputed datasets

``` {r corr_pool, eval=FALSE}

# load hetcor results
load(paste0(path, "/results/correlation/hetcor_output_sept_08_2022_all_data.RData"))

library(DescTools)

# extract matrices
corr <- lapply(corr_all_data, function(x) x[["correlations"]])

### transform coefficients using fisher's z scale transformation

# run Fisher transformation
corr_zs <- lapply(corr, FisherZ)

# replace inf with 100 (will re-transform back to 1)
corr_zs[[1]][is.infinite(corr_zs[[1]])] <- 100
corr_zs[[2]][is.infinite(corr_zs[[2]])] <- 100
corr_zs[[3]][is.infinite(corr_zs[[3]])] <- 100
corr_zs[[4]][is.infinite(corr_zs[[4]])] <- 100
corr_zs[[5]][is.infinite(corr_zs[[5]])] <- 100

# pool coefficients (which is just the average)
tmp <- Reduce('+', corr_zs)
corr_zs_pool <- tmp / length(corr_zs)

# transform coefficients back to r scale
corr_pool <- FisherZInv(corr_zs_pool)

### calculate p-value for all coefficients using z transformed coefficients and standard error. 
# See here for code suggestion: 
# https://stats.stackexchange.com/questions/61026/can-p-values-for-pearsons-correlation-test-be-computed-just-from-correlation-co

zse <- 1 / sqrt(nrow(all_data[[1]]) - 3) # standard error using all_data sample size
p.val_corr <- apply(corr_zs_pool, # z-transformed correlation coefficients
                    c(1,2), # apply below formula to cells in all rows and columns
                    function(x) min(pnorm(x, sd = zse),
                                    pnorm(x, lower.tail = F, sd = zse)) * 2) 

# save
save(corr_pool, 
     p.val_corr, 
     file = paste0(path, "/results/correlation/pooled_correlation_sept_08_2022_all_data.RData"))
```

## Reduce correlation matrix
## Variables replicated in the mortality XWAS and not sensitive to reverse causation bias

```{r load_correlation}

# load pooled XWAS reults
load(paste0(path,"/results/full cohort/ACM_XWAS_results_sept_07_2022_all_sexes_summary.RData"))
# load pooled correlation
load(paste0(path, "/results/correlation/pooled_correlation_sept_08_2022_all_data.RData"))
# load list of vars not significant in disease sensitivity
combined <- read.csv(paste0(path, "/output/healthy subset/ACM_XWAS_sensitivity_non_sig_vars_list_sept_07_2022_all_sexes.csv"))
combined <- as.vector(combined[[2]])

library(Hmisc)
library(factoextra)

# get vector of variables significant in EWAS + covariates
exposures <- XWAS_total[which(XWAS_total$FDR.rep < 0.05), ]
exposures <- unique(exposures$Variable)
exposures <- c(
  exposures,
  Cs(
    education_years,
    ethnicity
  )
)

# remove those flagged in sensitivity analysis
exposures <- exposures[exposures %nin% combined]

# subset correlation matrix to variables significant in XWAS
corr_pool <- as.data.frame(corr_pool)

corr_pool_sig <- corr_pool[ ,colnames(corr_pool) %in% exposures]
corr_pool_sig <- corr_pool_sig[rownames(corr_pool_sig) %in% exposures, ]

# round to 2 digits
corr_pool_sig <- as.matrix(corr_pool_sig)
corr_pool_sig <- round(corr_pool_sig, digits = 2)

```

## Calculating optimal number of clusters - AIC and WSS

```{r cluster_number}
## Determining optimal number of clusters by testing AIC of differnt K-means clustering runs
## Taken from: http://sherrytowers.com/2013/10/24/k-means-clustering/ 

# run kmeans clustering from 1 - 100 clusters
kmax = 100 # the maximum number of clusters we will examine; you can change this
totwss = rep(0, kmax) # will be filled with total sum of within group sum squares
kmfit = list() # create and empty list

set.seed(3456)
for (i in 1:kmax){
kclus = kmeans(corr_pool_sig,
               centers = i,
               iter.max = 20)
totwss[i] = kclus$tot.withinss
kmfit[[i]] = kclus
}

# function to calculate AIC
kmeansAIC = function(fit){

m = ncol(fit$centers)
n = length(fit$cluster)
k = nrow(fit$centers)
D = fit$tot.withinss
return(D + 2*m*k)

}

# run AIC
aic = sapply(kmfit, kmeansAIC)

# calculate optimal # of clusters and add to plot
v = -diff(aic)
nv = length(v)
fom = v[1:(nv-1)]/v[2:nv]
nclus = which.max(fom) + 1 # optimal number of clusters
cat("The apparent number of clusters is: ", nclus, "\n")

# plot and save
setwd(paste0(path, "/output/correlation"))
jpeg(paste0(paste0("AIC_exposome_", nclus), "_clusters_sept_16_2022_all_data.jpg"), 
    height = 7, 
    width = 15,
    units = "in", 
    res = 1200) 
# initialize plot (must run all code from here to end of chunk at same time)
plot(
    seq(1, kmax),
    aic,
    xlab = "Number of k-means clusters",
    ylab = "AIC of each cluster model",
    main = "Optimal Number of Clusters - Exposome Variables",
    font.main = 1,
    pch = 20,
    cex = 2
) 
# axis scale and intervals
axis(side = 1, at = seq(0, 100, by = 2))
# vertical line for number of clusters
abline(v = nclus, col = "black", lwd = 1, lty = 2)
# extra plot info
points(nclus,
       aic[nclus],
       col = 2,
       pch = 20,
       cex = 2)
dev.off()

## visualize optimal number of clusters using elbow method (sum of WSS)
## More tips here: https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/

elbow <- fviz_nbclust(corr_pool_sig, 
                      hcut, 
                      k.max = 100,
                      method = "wss") +
    geom_vline(xintercept = nclus, linetype = 2, color = "black") +
    geom_vline(xintercept = 8, linetype = 2, color = "red") +
    geom_vline(xintercept = 6, linetype = 2, color = "black") +
    labs(subtitle = "Elbow method")

setwd(paste0(path, "/output/correlation"))
jpeg(paste0(paste0("elbow_exposome_", nclus), "_clusters_sept_16_2022_all_data.jpg"), 
    height = 7, 
    width = 15,
     units = "in", 
    res = 1200) 
elbow +
    scale_x_discrete(breaks = seq(0,25,2)) +
    scale_y_continuous(breaks = seq(0,750,50)) +
    labs(y = "Total within-cluster sum of squares (WSS)") +
    theme(axis.title = element_text(size = 12)) +
    ggtitle("Optimal Number of Clusters - Exposome Variables")
dev.off()
```

## Plotting cluster solution using K-means

```{r cluster_number_plot}

set.seed(124)
res.km = kmeans(corr_pool_sig,
                centers = 6,
                iter.max = 100,
                nstart = 20)

nclus <- 6
pal <- palette("R4")[2:7]
names(pal) <- NULL

setwd(paste0(path, "/output/correlation"))
jpeg(paste0(paste0("kmeans_exposome_", nclus), "_clusters_sept_16_2022_all_data.jpg"),
    height = 5, 
    width = 7,
    units = "in", 
    res = 1200) 
# bottom, left, top, right
par(oma = c(3, 3, 3, 20)) # all sides have 3 lines of space
par(mar = c(3, 3, 3, 10), xpd = NA)
fviz_cluster(res.km, 
             data = corr_pool_sig,
             palette = pal,
             # geom = c("point", "text"),
             geom = c("point"),
             shape = 16,
             ellipse.type = "convex", 
             ggtheme = theme_classic(),
             xlab = "Principal component  1",
             ylab = "Principal component 2",
             main = NULL,
             legend = "none"
) + theme(plot.margin = margin(35, 35, 10, 10, "pt"))
dev.off()

```

## Format pretty variable names

```{r pretty_labels_beta}

# make cluster object
hr <- dist(corr_pool_sig, method = "euclidean")
fit <- hclust(hr, method = "complete")

# set labels
labels_df <- as.data.frame(fit$labels)
names(labels_df)[1] <- "labels"
labels_df$pretty_labels <- labels_df$labels


# make pretty labels for plot
# this section will need to be tweaked for each new set of results
labels_df$pretty_labels <-
    gsub("_England", "", 
         labels_df$pretty_labels, 
         fixed = TRUE)
labels_df$pretty_labels <-
    gsub("_", " ",
         labels_df$pretty_labels,
         fixed = TRUE)
labels_df$pretty_labels <-
    gsub("hshld", "household", 
         labels_df$pretty_labels, 
         fixed = TRUE)
labels_df$pretty_labels <-
    gsub("freq", "frequency", 
         labels_df$pretty_labels, 
         fixed = TRUE)
labels_df$pretty_labels <-
    gsub("prop", "prop.",
         labels_df$pretty_labels,
         fixed = TRUE)
labels_df$pretty_labels <- 
    gsub("pack", "smoking pack",
         labels_df$pretty_labels,
         fixed = TRUE)
labels_df$pretty_labels <- 
    gsub("tobacco", "tobacco use",
         labels_df$pretty_labels,
         fixed = TRUE)
labels_df$pretty_labels <- 
    gsub("environ", "environment",
         labels_df$pretty_labels,
         fixed = TRUE)
labels_df$pretty_labels <- 
    gsub("10yrs", "10 yrs",
         labels_df$pretty_labels,
         fixed = TRUE)

labels_df$pretty_labels[labels_df$labels == "recruitment_centre"] <- 
    "UKB assessment center"

labels_df$pretty_labels[labels_df$labels == "worry_embarassment"] <- 
    "worry after embarassment"

labels_df$pretty_labels[labels_df$labels == "PM10_2007"] <- 
    "PM10 pollution 2007"
labels_df$pretty_labels[labels_df$labels == "PM10_2010"] <- 
    "PM10 pollution 2010"

labels_df$pretty_labels[labels_df$labels == "NO2_2005"] <- 
    "NO2 pollution 2005"
labels_df$pretty_labels[labels_df$labels == "NO2_2006"] <- 
    "NO2 pollution 2006"
labels_df$pretty_labels[labels_df$labels == "NO2_2007"] <- 
    "NO2 pollution 2007"
labels_df$pretty_labels[labels_df$labels == "NO2_2010"] <- 
    "NO2 pollution 2010"

labels_df$pretty_labels[labels_df$labels == "NO_2010"] <- 
    "NO pollution 2010"

labels_df$pretty_labels[labels_df$labels == "NO2_2010"] <- 
    "NO2 pollution 2010"

labels_df$pretty_labels[labels_df$labels == "PM2.5_2010"] <- 
    "PM2.5 pollution 2010"

labels_df$pretty_labels[labels_df$labels == "psychiatrist_visit_mental_health"] <- 
    "psychiatrist visit for MH"
labels_df$pretty_labels[labels_df$labels == "doctor_visit_mental_health"] <- 
    "doctor visit for MH"

labels_df$pretty_labels[labels_df$labels == "maternal_smoking"] <- 
    "maternal smoking around birth"

labels_df$pretty_labels[labels_df$labels == "open_fire_heat"] <- 
    "uses open fire for heating"
labels_df$pretty_labels[labels_df$labels == "gas_hob_heat"] <- 
    "uses gas hob for heating"
labels_df$pretty_labels[labels_df$labels == "gas_fire_heat"] <- 
    "uses gas fire for heating"

labels_df$pretty_labels[labels_df$labels == "sound_average_16hr"] <- 
    "average noise pollution (16hr)"
labels_df$pretty_labels[labels_df$labels == "sound_average_24hr"] <- 
    "average noise pollution (24hr)"

labels_df$pretty_labels[labels_df$labels == "night_sound_average"] <- 
    "average night noise pollution"
labels_df$pretty_labels[labels_df$labels == "daytime_sound_average"] <- 
    "average daytime noise pollution"
labels_df$pretty_labels[labels_df$labels == "evening_sound_average"] <- 
    "average evening noise pollution"

labels_df$pretty_labels[labels_df$labels == "greenspace_buffer_300m"] <- 
    "greenspace (300m buffer)"
labels_df$pretty_labels[labels_df$labels == "greenspace_buffer_1000m"] <- 
    "greenspace (1000m buffer)"
labels_df$pretty_labels[labels_df$labels == "natural_environment_buffer_300m"] <- 
    "natural environment (300m buffer)"
labels_df$pretty_labels[labels_df$labels == "natural_environment_buffer_1000m"] <- 
    "natural environment (1000m buffer)"
labels_df$pretty_labels[labels_df$labels == "domestic_garden_buffer_300m"] <- 
    "domestic garden (300m buffer)"

labels_df$pretty_labels[labels_df$labels == "road_length_sum_100m"] <- 
    "sum of major roads length w/in 100m"

labels_df$pretty_labels[labels_df$labels == "public_transport_4wks"] <- 
    "public transport past 4 wks"
labels_df$pretty_labels[labels_df$labels == "cycle_transport_4wks"] <- 
    "cycle transport past 4 wks"
labels_df$pretty_labels[labels_df$labels == "walk_transport_4wks"] <- 
    "walking transport past 4 wks"
labels_df$pretty_labels[labels_df$labels == "car_transport_4wks"] <- 
    "car transport past 4 wks"


labels_df$pretty_labels[labels_df$labels == "strenuous_sports_4wks"] <- 
    "strenuous sports past 4wks"
labels_df$pretty_labels[labels_df$labels == "heavy_DIY_4wks"] <- 
    "heavy DIY past 4 wks (e.g., carpentry, digging)"
labels_df$pretty_labels[labels_df$labels == "light_DIY_4wks"] <- 
    "light DIY past 4 wks (e.g., pruning, watering lawn)"

labels_df$pretty_labels[labels_df$labels == "mobile_phone_duration"] <- 
    "years using mobile phone"
labels_df$pretty_labels[labels_df$labels == "mobile_phone_2yrs"] <- 
    "mobile phone use now vs. 2 yrs ago"

labels_df$pretty_labels[labels_df$labels == "vigorous_activity_over_10mins_days"] <- 
    "days/week 10+ minutes of vigorous PA"
labels_df$pretty_labels[labels_df$labels == "moderate_activity_over_10mins_days"] <- 
    "days/week 10+ minutes of moderate PA"
labels_df$pretty_labels[labels_df$labels == "mins_activity_sum"] <- 
    "summed mins of PA"

labels_df$pretty_labels[labels_df$labels == "sex_age_first"] <- 
    "age first sexual intercourse"

labels_df$pretty_labels[labels_df$labels == "easy_wake"] <- 
    "easy to wake in the morning"

labels_df$pretty_labels[labels_df$labels == "hshld_vehicles"] <- 
    "no. household vehicles"

labels_df$pretty_labels[labels_df$labels == "hshld_number"] <- 
    "no. people living in household"

labels_df$pretty_labels[labels_df$labels == "own_or_rent"] <- 
    "home ownership"
labels_df$pretty_labels[labels_df$labels == "other_exercise_4wks"] <- 
    "other exercise past 4wks (e.g., swim, cycle)"
labels_df$pretty_labels[labels_df$labels == "days_activity_sum"] <- 
    "summed days of PA"
labels_df$pretty_labels[labels_df$labels == "pleasure_walks_4wks"] <- 
    "pleasure walks past 4 wks"
labels_df$pretty_labels[labels_df$labels == "sleep_hours_categorical"] <- 
    "sleep hours"
labels_df$pretty_labels[labels_df$labels == "above_activity_recommendation"] <- 
    "meets UK PA guidelines"
labels_df$pretty_labels[labels_df$labels == "above_activity_recommendation_incl_walk"] <- 
    "meets UK PA guidelines (incl. walk)"
labels_df$pretty_labels[labels_df$labels == "walked_over_10mins_days"] <- 
    "no. days/week walked 10+ minutes"

labels_df$pretty_labels[labels_df$labels == "air_pollution_PC1"] <- 
    "air pollution"

labels_df$pretty_labels[labels_df$labels == "greenspace_PC1"] <- 
    "greenspace"

# set labels to new pretty labels
pretty_labels <- labels_df$pretty_labels
fit$labels <- pretty_labels
dend <- as.dendrogram(fit)

```

## Plot linear dendrogram

```{r linear_dendro}

library(randomcoloR)
library(dendextend)
library(dplyr)

# set number of clusters
nclus <- 8
pal <- palette.colors(palette = "Okabe-Ito")
pal <- pal[c(8,1:7)]

# plot
setwd(paste0(path, "/output/correlation"))
jpeg(paste0(paste0("linear_dendogram_exposome_", nclus), "_clusters_sept_28_2022_all_data_vertical.jpg"), 
    width = 10,
    height = 15,
    units = "in", 
    res = 1800) 
# bottom, left, top, right
par(oma = c(3, 3, 3, 3)) # all sides have 3 lines of space
par(mar = c(3, 3, 3, 10), xpd = NA)
dend %>% 
    set("branches_k_color", k = nclus, value = pal) %>%
    set("branches_lwd", 2) %>% 
    set("labels_cex", 0.65)  %>% 
    plot(horiz = TRUE)
dev.off()


```

## Save excel file of variables per cluster

```{r save_clusters}

# make cluster object
hr <- dist(corr_pool_sig, method = "euclidean")
fit <- hclust(hr, method = "complete")

# get df of clusters
groups <- cutree(fit, k = nclus)

# convert to df
mydata <- data.frame(groups)
  
# make column with variable names
mydata$Variable <- rownames(mydata)

# set column names
colnames(mydata) <- c("Cluster", "Variable")

# create list to store pool results
prefix <- "cluster"
suffix <- seq(1,nclus)
clusters <- as.list(paste0(prefix, suffix))

# loop to create list of each cluster
for(j in seq_along(clusters)){
 clusters[[j]] <-  mydata[mydata$Cluster == j,]
}

# remove cluster number
clusters <- lapply(clusters, function(x) x <- subset(x, select = c("Variable")))

# allow to be df with different length (need it double)
clusters <- sapply(clusters, "length<-", max(lengths(clusters)))
clusters <- sapply(clusters, "length<-", max(lengths(clusters)))
clusters <- as.data.frame(clusters)

prefix <- "Cluster"
suffix <- seq(1,nclus)
cluster_names <- paste(prefix, suffix, sep = " ")

colnames(clusters) <- c(cluster_names)

# save
write.csv(clusters, 
          file = paste0(path, paste0(paste0("/output/correlation/hierarchical_exposome_", nclus), "_clusters_sept_15_2022_all_data.csv")))


```

## output html correlation matrix

```{r save_html_correlation_matrix_all_vars}

# rename
corr <- corr_pool
corr <- as.matrix(corr)
corr <- round(corr, digits = 2)

#prepare to drop duplicates and correlations of 1     
corr[lower.tri(corr,diag = TRUE)] <- NA 
#turn into a 3-column table
corr <- as.data.frame(as.table(corr))
#remove the NA values from above 
corr <- na.omit(corr) 

# rename
p_sig <- p.val_corr

# Keep only upper triangle (bottom triangle are all duplicates)
p_sig[lower.tri(p_sig, diag = TRUE)] <- NA 
#turn into a 3-column table
p_sig <- as.data.frame(as.table(p_sig))
#remove the NA values from above 
p_sig <- na.omit(p_sig) 

corr <- cbind(corr, p_sig)
corr <- corr[, -(4:5)]

#sort by highest correlation
corr <- corr[order(-abs(corr$Freq)),] 

colnames(corr) <- c("Variable 1", "Variable 2", "Coefficient", "p-value")

library(DT)
plot1 <- datatable(corr,rownames = FALSE, fillContainer = T,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:2)),
                         scrollY = '600px',
                         scrollx = '100%',
                         pageLength = 25),
          caption = "Baseline Variable Correlation - UK Biobank") %>%
    formatRound(columns=c("p-value"), digits = 5)

# save
saveWidget(plot1, 
           file = paste0(path, "/output/correlation/html/correlation_table_all_vars_june_18_2022_all_sexes.html"))

```

## Count percentage of significant correlations

```{r sig_correlations_count}

## Transform correlation and p-value matrices

p_sig <- p.val_corr[rownames(corr_pool_sig),colnames(corr_pool_sig)]

# Keep only upper triangle (bottom triangle are all duplicates)
p_sig[lower.tri(p_sig, diag = TRUE)] <- NA
#turn into a 3-column table
p_sig <- as.data.frame(as.table(p_sig))
#remove the NA values from above 
p_sig <- na.omit(p_sig) 

corr <- corr_pool_sig
corr <- as.matrix(corr)

# Keep only upper triangle (bottom triangle are all duplicates)
corr[lower.tri(corr, diag = TRUE)] <- NA 
#turn into a 3-column table
corr <- as.data.frame(as.table(corr))
#remove the NA values from above 
corr <- na.omit(corr) 

bonferroni <- 0.05/(nrow(corr))

## get number of correlations by p-value significance

#select significant values  
p_sig.05 <- subset(p_sig, Freq < 0.05) 
p_sig.01 <- subset(p_sig, Freq < 0.01) 
p_sig.0001 <- subset(p_sig, Freq < 0.001) 
p_sig.bon <- subset(p_sig, Freq < bonferroni)

# calculate percent significantly correlated
percent_05 <- nrow(p_sig.05)/nrow(p_sig)
percent_01 <- nrow(p_sig.01)/nrow(p_sig)
percent_0001 <- nrow(p_sig.0001)/nrow(p_sig)
percent_bon <- nrow(p_sig.bon)/nrow(p_sig)

## get mean and median correlation coefficients by p-value significance

# calculate means 
corr_05 <- mean(abs(corr[rownames(p_sig.05),]$Freq))
corr_01 <- mean(abs(corr[rownames(p_sig.01),]$Freq))
corr_0001 <- mean(abs(corr[rownames(p_sig.0001),]$Freq))
corr_bon <- mean(abs(corr[rownames(p_sig.bon),]$Freq))

# calculate medians
med_05 <- median(abs(corr[rownames(p_sig.05),]$Freq))
med_01 <- median(abs(corr[rownames(p_sig.01),]$Freq))
med_0001 <- median(abs(corr[rownames(p_sig.0001),]$Freq))
med_bon <- median(abs(corr[rownames(p_sig.bon),]$Freq))

percent <- c(percent_05,percent_01,percent_0001,percent_bon)
cors <- c(corr_05,corr_01,corr_0001,corr_bon)
meds <- c(med_05,med_01,med_0001,med_bon)

# count correlations above 0.3
length <- ncol(corr_pool_sig)
pairs <- length * length
counts <- lapply(corr_pool_sig, function(x) abs(x) > 0.3)
sigs <- length(counts[counts == TRUE])
sig_pairs <- sigs/pairs

# count correlations above 0.5
counts_05 <- lapply(corr_pool_sig, function(x) abs(x) > 0.5)
sigs_05 <- length(counts_05[counts_05 == TRUE])
sig_pairs_05 <- sigs_05/pairs

table <- data.frame(matrix(NA, nrow = 6, ncol = 3))
table[1:4, 1] <- cors
table[1:4, 2] <- meds
table[1:4, 3] <- percent*100
table[5, 3] <- sig_pairs*100
table[6, 3] <- sig_pairs_05*100
table <- format(round(table, digits = 2), nsmall = 2) # round to 2 decimals and force to keep 2 decimals (e.g., .80 and not .8)
                   
rownames(table) <- c("p < 0.05", "p < 0.01", "p < 0.0001", "Bonferroni", "Coefficient above 0.30", "Coefficient above 0.50")
colnames(table) <- c("Mean coefficient (abs)", "Median coefficient (abs)", "Percent of correlations")
table$"Percent of correlations" <- paste(table$"Percent of correlations"," %")

# save
write.csv(table, 
          file = paste0(path, "/output/correlation/exposome_correlation_significance_all_sexes_sept_08_2022.csv"))

library(DT)
datatable(table,
          options = list(columnDefs = list(list(className = 'dt-left', targets = 0:3))))

```
